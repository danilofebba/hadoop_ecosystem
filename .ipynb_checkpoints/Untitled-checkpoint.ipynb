{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e483b9-916b-456a-8c15-8480da38a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snakebite.client import Client\n",
    "# the below line create client connection to the HDFS NameNode\n",
    "client = Client('192.168.43.2', 9000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f9287-a558-442d-ae64-4b82e9bddf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loop iterate in root directory to list all the content \n",
    "for x in client.ls(['/warehouse']):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1eb13-36ab-437e-9f17-5b1ef705b5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c428969-fefb-4cbc-8505-f575d6f5f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87ee89-fb4c-4468-9d53-28d4cd94cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"example-pyspark-read-and-write\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b480a4-ec81-49ec-8569-b02b26ebf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.createDataFrame([{\"id\":1,\"name\":\"danilo\"},{\"id\":2,\"name\":\"daniel\"},{\"id\":3,\"name\":\"luana\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0643342-22f8-405f-9a24-10c984e84edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"hdfs://hadoop:9000/warehouse/{0}\".format(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf80b78-bb6b-44e9-89b2-973acbffc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.csv('hdfs://192.168.43.2:9000/warehouse/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca4721-f876-420a-ab2e-2058481fc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3aaee-1116-402d-a45f-805b2194d8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f94745-cc9e-4078-a61d-5e35188b86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c009f-2401-4ba2-b5f5-0b1f8344574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pyhdfs.HdfsClient(hosts='192.168.43.2:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8959429-36c4-4e7f-91a8-506f01001375",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.copy_from_local(u'/home/danilo/Documents/warehouse/fact01/2023/1/data_file_20230101.parquet', u'/warehouse/fact01/2023/1/data_file_20230101.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529785e-72c9-4c16-a25d-2a165ad2f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import *\n",
    "client = Client(\"http://192.168.43.2:9000\")\n",
    "print(client.list('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f45516-9fa2-4f2d-b38b-689448cb6b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
