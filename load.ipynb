{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287900d-1b53-4852-b25b-6a9b83842764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt_BR.UTF-8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf7f3d6-de75-4a13-9ea6-3fec4b398b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e746dd-0352-4daf-b24d-89f12360104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f38dd-9502-4267-aafe-d7b950686396",
   "metadata": {},
   "source": [
    "# dimensao_calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab977a7-193f-4ee4-b3a4-650fc8c20418",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(365):\n",
    "    d = datetime.datetime(2021,1,1) + datetime.timedelta(days=i)\n",
    "    spark.sql(\"\"\"\n",
    "        insert into dimensao_calendario partition (\n",
    "            ano = {2},\n",
    "            mes = {3}\n",
    "        ) values (\n",
    "            {0},\n",
    "            date '{1}',\n",
    "            {4}\n",
    "        )\n",
    "    \"\"\".format(d.strftime('%Y%m%d'), d.strftime('%Y-%m-%d'), d.strftime('%Y'), int(d.strftime('%m')), int(d.strftime('%d'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3529ee7-4736-40f2-929c-d0236e761c80",
   "metadata": {},
   "source": [
    "# dimensao_semana_ano_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11ba64f-0348-4a38-b8ad-947f78747a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(365):\n",
    "    d = datetime.datetime(2021,1,1) + datetime.timedelta(days=i)\n",
    "    if int(d.strftime('%G%V')) not in data:\n",
    "        data.append(int(d.strftime('%G%V')))\n",
    "        spark.sql(\"\"\"\n",
    "            insert into dimensao_semana_ano_iso partition (\n",
    "                ano_iso = {0}\n",
    "            ) values (\n",
    "                {1},\n",
    "                {2},\n",
    "                '{3}'\n",
    "            )\n",
    "        \"\"\".format(int(d.strftime('%G')), int(d.strftime('%G%V')), int(d.strftime('%V')), d.strftime('%G-S%V')))\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27f7af-e947-425c-96b6-af0a8abe912f",
   "metadata": {},
   "source": [
    "# dimensao_mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a23e518-2921-4cdb-9217-8ddc99904956",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(365):\n",
    "    d = datetime.datetime(2021,1,1) + datetime.timedelta(days=i)\n",
    "    if int(d.strftime('%Y%m')) not in data:\n",
    "        data.append(int(d.strftime('%Y%m')))\n",
    "        spark.sql(\"\"\"\n",
    "            insert into dimensao_mes partition (\n",
    "                ano = {0}\n",
    "            ) values (\n",
    "                {1},\n",
    "                {2},\n",
    "                '{3}'\n",
    "            )\n",
    "        \"\"\".format(int(d.strftime('%Y')), int(d.strftime('%Y%m')), int(d.strftime('%m')), d.strftime('%b/%Y')))\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fd86d-d60b-431c-8e6b-abcf67387cc0",
   "metadata": {},
   "source": [
    "# dimensao_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03fd948-894d-42ab-88e9-7db51cf5cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_excel('exemplos.xlsx', sheet_name='dimensao_eps', header=0)\n",
    "for i, r in df.iterrows():\n",
    "    spark.sql(\"\"\"\n",
    "        insert into dimensao_eps values ({0}, '{1}', '{2}')\n",
    "    \"\"\".format(r['sk_eps'], r['nome_eps'], r['sigla_eps']))\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76f463-30bb-491f-b281-8dca2ffee8d1",
   "metadata": {},
   "source": [
    "# dimensao_celula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bad6007-0cf6-4fd9-9a3d-f7a3139c5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_excel('exemplos.xlsx', sheet_name='dimensao_celula', header=0)\n",
    "for i, r in df.iterrows():\n",
    "    spark.sql(\"\"\"\n",
    "        insert into dimensao_celula values ({0}, '{1}')\n",
    "    \"\"\".format(r['sk_celula'], r['nome_celula']))\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45a16b-3be5-4fb3-83c9-3ee6f3f59377",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5a2c0-2c79-4278-a81a-107fd103529e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
